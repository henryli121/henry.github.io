<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/henry.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/henry.github.io/" rel="alternate" type="text/html" /><updated>2024-07-12T20:08:38+08:00</updated><id>http://localhost:4000/henry.github.io/feed.xml</id><title type="html">My Approximation</title><subtitle>&quot;在终极的分析中，一切知识都是历史；在抽象的意义下，一切科学都是数学；在理性的基础上，所有的判断都是统计学&quot;。
&lt;br&gt;
“All models are wrong, but some are useful&quot; -- George E.P.Box
</subtitle><entry xml:lang="en"><title type="html">Identification of Nonlinear Dynamics (SINDy) to Time-Series Data</title><link href="http://localhost:4000/henry.github.io/research/2024/07/11/01/" rel="alternate" type="text/html" title="Identification of Nonlinear Dynamics (SINDy) to Time-Series Data" /><published>2024-07-11T19:00:00+08:00</published><updated>2024-07-11T19:00:00+08:00</updated><id>http://localhost:4000/henry.github.io/research/2024/07/11/01</id><content type="html" xml:base="http://localhost:4000/henry.github.io/research/2024/07/11/01/"><![CDATA[<div class="quote-box">
  <p><b>Introduction</b></p>
</div>

<p>This project employs the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm to identify governing equations from time-series data of three features related to fluid and gas dynamics. The data is from my previous project titled “<a href="/henry.github.io/assets/papers/Backpropagation.pdf">Enhanced Feature Prediction in Time-Series Data Using a Structured Learning Approach</a>”. The three features analyzed are: Velocity of Fluid at Output 1, Mass Flow Rate of Gas at Output 1, and Velocity of Gas at Output 2. Using the SINDy algorithm, we reconstruct the underlying mathematical models governing these features by constructing a library of candidate functions, using sparse regression to identify the most relevant terms, and validating the identified equations against the observed data. This project demonstrates the effectiveness of SINDy in discovering simple, interpretable models from complex time-series data, enhancing our understanding of fluid and gas dynamics.</p>

<div class="quote-box">
  <p><b>Model Explanation: Sparse Identification of Nonlinear Dynamics (SINDy) Algorithm</b></p>
</div>

<p>Sparse Identification of Nonlinear Dynamics (SINDy) is an algorithm designed to discover parsimonious models that describe dynamical systems using a small number of terms from a large library of candidate functions. The approach combines the principles of sparse regression with a systematic library of nonlinear functions to identify the governing equations of the system from time-series data.</p>

<h3 id="mathematical-formulation">Mathematical Formulation</h3>

<p>Consider a dynamical system described by the ordinary differential equation (ODE):</p>

\[\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x})\]

<p>where \(\mathbf{x} \in \mathbb{R}^n\) is the state vector of the system and \(\mathbf{f}\) represents the nonlinear dynamics.</p>

<p>The goal of SINDy is to approximate the function \(\mathbf{f}\) by a sparse linear combination of candidate functions from a predefined library. The process involves the following steps:</p>

<ol>
  <li>
    <p><strong>Data Collection</strong>:
Collect time-series data for the state variables \(\mathbf{x}(t)\). Let \(\mathbf{X}\) be the matrix containing these measurements, where each column corresponds to a different state variable and each row corresponds to a different time point.</p>
  </li>
  <li>
    <p><strong>Library Construction</strong>:
Construct a library \(\mathbf{\Theta}(\mathbf{X})\) of candidate functions. The library includes various nonlinear functions of the state variables, such as polynomials, trigonometric functions, and exponentials. Mathematically, the library can be represented as:</p>

\[\mathbf{\Theta}(\mathbf{X}) = [\mathbf{1}, \mathbf{X}, \mathbf{X}^2, \sin(\mathbf{X}), \cos(\mathbf{X}), \exp(\mathbf{X}), \mathbf{X} \exp(\mathbf{X}), \ldots],\]

    <p>where each column of \(\mathbf{\Theta}\) represents a different candidate function.</p>
  </li>
  <li>
    <p><strong>Sparse Regression</strong>:
For each state variable \(x_i\) express the time derivative \(\frac{dx_i}{dt}\) as a linear combination of the candidate functions in \(\mathbf{\Theta}(\mathbf{X})\):</p>

\[\frac{d\mathbf{X}}{dt} = \mathbf{\Theta}(\mathbf{X}) \mathbf{\Xi},\]

    <p>where \(\mathbf{\Xi}\) is a matrix of coefficients, with each column corresponding to a different state variable. The key idea is to enforce sparsity in \(\mathbf{\Xi}\), meaning that most of the coefficients should be zero. This can be achieved using sparse regression techniques such as the Least Absolute Shrinkage and Selection Operator (LASSO):</p>

\[\mathbf{\Xi} = \underset{\mathbf{\Xi}}{\operatorname{argmin}} \left\| \frac{d\mathbf{X}}{dt} - \mathbf{\Theta}(\mathbf{X}) \mathbf{\Xi} \right\|_2^2 + \lambda \left\| \mathbf{\Xi} \right\|_1,\]

    <p>where \(\lambda\) is a regularization parameter that controls the sparsity of the solution.</p>
  </li>
  <li>
    <p><strong>Model Identification</strong>:
The non-zero entries in the coefficient matrix \(\mathbf{\Xi}\) correspond to the terms in the library \(\mathbf{\Theta}(\mathbf{X})\) that are most relevant for describing the dynamics of the system. The identified model can be written as:</p>

\[\frac{dx_i}{dt} = \sum_{j} \xi_{ij} \theta_j(\mathbf{x}),\]

    <p>where \(\xi_{ij}\) are the non-zero coefficients and \(\theta_j(\mathbf{x})\) are the corresponding candidate functions.</p>
  </li>
</ol>

<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0711_1.JPG" alt="Image 1" />
   <br />
   <div class="caption">Schematic of the SINDy algorithm, demonstrated on the Lorenz equations</div>
</div>

<p>In summary, SINDy offers a powerful framework for identifying governing equations from data, providing insights into the dynamics of complex systems through sparse and interpretable models. You can read original SINDy Paper <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.1517384113">here</a>.</p>

<div class="quote-box">
  <p><b>Data Processing</b></p>
</div>

<h3 id="preliminary">Preliminary</h3>

<p>The data analyzed in this project stems from a previous machine learning effort titled <a href="/henry.github.io/assets/papers/Backpropagation.pdf">“Enhanced Feature Prediction in Time-Series Data Using a Structured Learning Approach”</a>. This earlier project aimed to establish statistical relationships between selected features \(X = (X_1, X_2)\) and target features \(Y = (Y_1, Y_2, Y_3)\) using a combination of different models. The learning architecture involved three main stages: initial prediction, residual training, and model refinement. Through this parameter-focused approach, we obtained a prediction called \(Y_{\text{final}}\) of the target features with a relative MSE of 0.0005. However, these numerical predictions lack explanations and mathematical reasoning, especially in the last stage where we used a Multi-Layer Perceptron (MLP), which is often criticized as being a black box.</p>

<p>Hence, in this project, I aim to use SINDy to construct the approximation of governing equations for these results. Although the approximated governing equations may not provide results as accurate as those from the numerical model, I hope they can offer more mathematical reasoning for the results and enhance our understanding of fluid and gas dynamics.</p>

<p>The three features analyzed in the current SINDy project are:</p>

<ol>
  <li>
    <p><strong>Velocity of Fluid at Output 1</strong>: This feature represents the speed at which the fluid is moving at the first output point. It is crucial for understanding the dynamics and behavior of fluid flow within the system.</p>
  </li>
  <li>
    <p><strong>Mass Flow Rate of Gas at Output 1</strong>: This feature measures the mass of gas passing through the first output point per unit time. It is essential for evaluating the efficiency and performance of gas transport within the system.</p>
  </li>
  <li>
    <p><strong>Velocity of Gas at Output 2</strong>: This feature indicates the speed of gas flow at the second output point. It provides insights into the behavior of gas dynamics, which is critical for optimizing and controlling gas-related processes.</p>
  </li>
</ol>

<p>These features are recorded over a series of time steps, resulting in three columns of time-series data.</p>

<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0711_2.JPG" alt="Image 2" />
   <br />
   <div class="caption">Time-series plot for the three features</div>
</div>

<h3 id="candidate-choice">Candidate Choice</h3>

<p>To identify the governing equations for the three features, we construct a library of candidate functions. The chosen candidates are designed to capture a wide range of potential dynamics within the system. The library includes:</p>

<ol>
  <li>
    <p><strong>Constant term</strong>: Represents a constant value across all time steps, capturing any baseline level in the data. This term helps to account for any steady-state behavior or offset in the data.</p>
  </li>
  <li>
    <p><strong>Linear term \(t\)</strong>: Captures any linear trend in the data over time. This term is important for identifying any steady increase or decrease in the values, which might correspond to consistent acceleration or deceleration in the system.</p>
  </li>
  <li>
    <p><strong>Quadratic term \(t^2\)</strong>: Accounts for any quadratic or parabolic trends in the data. This term is useful for modeling acceleration or deceleration that changes over time, often seen in systems experiencing non-linear growth or decay.</p>
  </li>
  <li>
    <p><strong>Exponential term \(\exp(-t)\)</strong>: Models exponential decay, which is common in many physical and chemical processes such as radioactive decay, cooling, or damping in mechanical systems. The shape of this function rapidly decreases to zero as \(t\) increases, making it ideal for capturing processes that diminish over time.</p>
  </li>
  <li>
    <p><strong>Sine term \(\sin(t)\)</strong>: Captures periodic oscillations or cycles in the data, often present in systems with harmonic motion like waves, vibrations, or seasonal variations. The sine function is valuable for identifying cyclic behaviors that repeat over regular intervals.</p>
  </li>
  <li>
    <p><strong>Cosine term \(\cos(t)\)</strong>: Complements the sine term, capturing phase-shifted periodic behavior. While the sine term starts at zero, the cosine term starts at one, allowing for the capture of oscillatory behavior with different phases.</p>
  </li>
  <li>
    <p><strong>Interaction term \(t \cdot \exp(-t)\)</strong>: Represents a combination of linear and exponential dynamics, useful for modeling more complex interactions in the system. This term starts at zero, increases to a peak, and then decays, making it suitable for processes that initially grow and then decay over time.</p>
  </li>
</ol>

<p>These candidate functions are selected to provide a comprehensive while relatively simple basis for approximating the governing equations. Of course, you can select as many complex terms as you want to get a closer approximation; however, it is subtle to choose between interpretability and over-fitting.</p>

<div class="quote-box">
  <p><b>Network Setup and Training</b></p>
</div>

<ol>
  <li>
    <p><strong>Library Construction</strong>:
A library of candidate functions is constructed for each feature. The library includes constant, linear, quadratic, exponential, sine, cosine, and interaction terms. These candidate functions are chosen to capture a wide range of potential dynamics within the system.</p>
  </li>
  <li><strong>Data Splitting</strong>:
The data for each feature is split into three intervals:
    <ul>
      <li>Interval 1: \(t \in [0, 10)\)</li>
      <li>Interval 2: \(t \in [10, 600)\)</li>
      <li>Interval 3: \(t \in [600, 2000]\)</li>
    </ul>

    <p>Splitting the data into these intervals allows us to capture different dynamic behaviors that may occur over different time periods. Each interval is treated separately to ensure that the governing equations identified are specific to the dynamics within that interval, providing a more accurate and detailed understanding of the system’s behavior over time.</p>
  </li>
  <li><strong>Sparse Regression</strong>:
The LassoCV algorithm, a type of L1-regularized linear regression, is used to fit the data. This process involves:
    <ul>
      <li>Scaling the candidate functions to standardize the data.</li>
      <li>Performing cross-validation to determine the optimal regularization parameter (\(\lambda\)).</li>
      <li>Identifying the coefficients for the most significant terms in the governing equations, ensuring that the resulting model is both sparse and interpretable.</li>
    </ul>
  </li>
  <li><strong>Equation Construction</strong>:
For each interval and each feature, the identified coefficients and terms are combined to form the governing equations. These piecewise equations provide a detailed mathematical representation of the dynamics within each interval, enhancing our understanding of the fluid and gas dynamics by revealing the underlying processes that drive the observed behavior.</li>
</ol>

<div class="quote-box">
  <p><b>Experiment Results</b></p>
</div>
<p>The experimenting results are showing in the plot below:</p>

<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0711_3.JPG" alt="Image 3" />
   <br />
   <div class="caption">Identified govern-equations and their corresponding plots vs actual results</div>
</div>
<p>The experiment results compare the actual time-series data for each feature with the identified equations derived using the SINDy algorithm. The three features analyzed are: <i>Velocity of Fluid at Output 1</i>, <i>Mass Flow Rate of Gas at Output 1</i>, and <i>Velocity of Gas at Output 2</i>.</p>

<p>Overall, the identified equations capture the general trends and dynamic behaviors of the actual data. The SINDy algorithm effectively balances simplicity (interpretability) and accuracy (avoiding overfitting) by selecting a limited number of significant terms for the governing equations. This balance ensures that the model remains interpretable while providing a good approximation of the underlying dynamics.</p>

<p>In the plots, the identified piecewise equations are presented alongside the actual data. These equations offer a detailed mathematical representation of the system’s behavior within each interval. While there are some deviations between the identified and actual data, particularly in certain intervals, the overall fit is strong, demonstrating the effectiveness of the SINDy approach in capturing the essential dynamics of fluid and gas behaviors.</p>

<div class="quote-box">
  <p><b>Conclusion</b></p>
</div>
<p>In this project, we applied the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm to identify governing equations for time-series data from a previous machine learning project. Several key points and reflections emerged from this study:</p>

<ol>
  <li>
    <p><strong>Candidate Choice is the Key</strong>:
Selecting appropriate candidate functions to build the library is the most crucial part of SINDy. The library directly impacts the effectiveness and accuracy of the identified equations. A well-chosen library allows the model to capture the essential dynamics of the system while maintaining interpretability.</p>
  </li>
  <li>
    <p><strong>Prior Knowledge of the System</strong>:
The success of SINDy heavily relies on prior knowledge of the system to select the right candidates. This presents a challenge in real-world scenarios where background information about the data may be limited or unavailable. The effectiveness of SINDy in such cases remains an open question, highlighting the need for further research and potential enhancements to the algorithm.</p>
  </li>
  <li>
    <p><strong>Balance Between Simplicity and Accuracy</strong>:
Achieving a balance between simplicity and accuracy is a subtle and critical step in SINDy. While adding more complex candidate functions can improve approximation accuracy, it may lead to overfitting and result in terms that are difficult to interpret physically. For instance, a term like \(\sin(\exp(t^{\tan(\exp(t^4))}))\) might improve fit but lacks physical meaning and interpretability. The goal is to find a middle ground where the model is both accurate and interpretable, providing meaningful insights into the system dynamics.</p>
  </li>
  <li>
    <p><strong>Code Repository:</strong> The complete code for this project is available on <a href="https://github.com/henryli121/SINDy-govern-equation-.git">GitHub</a>.</p>
  </li>
</ol>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry xml:lang="en"><title type="html">Enhanced Feature Prediction in Time-Series Data Using a Structured Learning Approach</title><link href="http://localhost:4000/henry.github.io/paper/2024/07/09/01/" rel="alternate" type="text/html" title="Enhanced Feature Prediction in Time-Series Data Using a Structured Learning Approach" /><published>2024-07-09T22:10:00+08:00</published><updated>2024-07-09T22:10:00+08:00</updated><id>http://localhost:4000/henry.github.io/paper/2024/07/09/01</id><content type="html" xml:base="http://localhost:4000/henry.github.io/paper/2024/07/09/01/"><![CDATA[]]></content><author><name></name></author><category term="paper" /><summary type="html"><![CDATA[]]></summary></entry><entry xml:lang="en"><title type="html">Contrastive Out-of-Distribution with Multi-modal Information for Document Classification</title><link href="http://localhost:4000/henry.github.io/paper/2024/06/19/02/" rel="alternate" type="text/html" title="Contrastive Out-of-Distribution with Multi-modal Information for Document Classification" /><published>2024-06-19T20:10:00+08:00</published><updated>2024-06-19T20:10:00+08:00</updated><id>http://localhost:4000/henry.github.io/paper/2024/06/19/02</id><content type="html" xml:base="http://localhost:4000/henry.github.io/paper/2024/06/19/02/"><![CDATA[]]></content><author><name></name></author><category term="paper" /><summary type="html"><![CDATA[]]></summary></entry><entry xml:lang="en"><title type="html">Training a Machine to Learn Different Languages</title><link href="http://localhost:4000/henry.github.io/research/2024/06/19/01/" rel="alternate" type="text/html" title="Training a Machine to Learn Different Languages" /><published>2024-06-19T20:00:00+08:00</published><updated>2024-06-19T20:00:00+08:00</updated><id>http://localhost:4000/henry.github.io/research/2024/06/19/01</id><content type="html" xml:base="http://localhost:4000/henry.github.io/research/2024/06/19/01/"><![CDATA[<div class="quote-box">
  <p><b>Introduction</b></p>
</div>

<p>Understanding and identifying languages from text inputs is a fundamental task in natural language processing, with applications in automatic translation and multilingual sentiment analysis. This project aims to develop a machine learning model to accurately identify the language of a given text snippet. We leverage Long Short-Term Memory (LSTM) networks, a type of recurrent neural network known for capturing long-term dependencies in sequential data. LSTMs maintain context over extended text sequences, making them ideal for understanding and processing human languages. This post details the process from data preparation and cleaning to training an LSTM-based model.</p>

<div class="quote-box">
  <p><b>Model Architecture: Long Short-Term Memory (LSTM)</b></p>
</div>

<p>Long Short-Term Memory (LSTM) networks are designed to address the vanishing gradient problem inherent in traditional RNNs, making them effective for handling long-term dependencies in sequential data.</p>

<p>The architecture of an LSTM cell involves a gating mechanism that includes a forget gate, an input gate, and an output gate. These gates regulate information flow, enabling the cell to maintain and update information over long sequences.</p>

<p>The internal operations of an LSTM cell can be described by the following equations:</p>

<ol>
  <li>
    <p><strong>Forget Gate:</strong> Determines which information from the previous cell state to discard:
\(f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)\)</p>
  </li>
  <li>
    <p><strong>Input Gate:</strong> Decides which new information to add to the cell state:
\(i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)\)
\(\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)\)</p>
  </li>
  <li>
    <p><strong>Cell State Update:</strong> Combines the old cell state, modified by the forget gate, with the new candidate values, scaled by the input gate:
\(C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t\)</p>
  </li>
  <li>
    <p><strong>Output Gate:</strong> Decides the next hidden state based on the updated cell state:
\(o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)\)
\(h_t = o_t \cdot \tanh(C_t)\)</p>
  </li>
</ol>

<p>In these equations:</p>
<ul>
  <li>\(\sigma\) represents the sigmoid activation function.</li>
  <li>\(\tanh\) denotes the hyperbolic tangent function.</li>
  <li>\(W\) and \(b\) are the weight matrices and biases.</li>
  <li>\(h_t\) and \(C_t\) represent the hidden state and cell state at time step \(t\).</li>
</ul>

<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0620_1.JPG" alt="Image 1" />
   <br />
   <div class="caption">Figure 1: Diagram illustration of LSTM</div>
</div>

<p>LSTMs excel in language tasks due to their ability to retain context over extended sequences, making them a preferred choice for NLP applications such as language modeling, translation, and sentiment analysis.</p>

<div class="quote-box">
  <p><b>Data Collection and Cleaning</b></p>
</div>

<p>The initial step involved preparing a dataset comprising text samples from various languages. Ensuring the data was clean and well-structured was crucial for the model’s performance.</p>

<p>The data cleaning process involved identifying and removing unlabeled data, handling books with unusual names, stripping prefaces and postfaces, and filtering outliers. After cleaning, the dataset was balanced by selecting the top six languages and structured into a 3D tensor. The final processed data was saved in <code class="language-plaintext highlighter-rouge">.npy</code> files for efficient training.</p>

<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0620_2.JPG" alt="Image 2" />
   <br />
   <div class="caption">Figure 2: The format of an ebook</div>
</div>

<h3 id="data-cleaning-process">Data Cleaning Process</h3>

<p>Several critical steps were involved to ensure the dataset’s integrity and usability:</p>

<ol>
  <li><strong>Identification and Removal of Unlabeled Data:</strong> Manually identifying and removing books without clear language tags.</li>
  <li><strong>Handling Unusual Names:</strong> Manually reviewing books with non-standard naming formats to maintain consistency.</li>
  <li><strong>Stripping Prefaces and Postfaces:</strong> Removing prefaces and postfaces typically written in English to prevent language contamination.</li>
  <li><strong>Filtering Outliers:</strong> Removing books that did not align with the typical format.</li>
</ol>

<p>After these steps, the dataset was reduced to 36,082 books. To ensure balanced training, the top six languages with the highest number of samples were selected.</p>

<h3 id="data-distribution-and-structure">Data Distribution and Structure</h3>

<p>Post-cleaning, the dataset was structured into a 3D tensor. For each language and each book, random sequences of 30 consecutive characters were extracted and tokenized using one-hot encoding.</p>

<p>The 3D tensor can be represented as follows:</p>

\[\mathcal{X} = 
\begin{bmatrix}
\begin{bmatrix}
x_{0,0} &amp; x_{0,1} &amp; x_{0,2} \\
x_{1,0} &amp; x_{1,1} &amp; x_{1,2} \\
x_{2,0} &amp; x_{2,1} &amp; x_{2,2} 
\end{bmatrix} \\
\begin{bmatrix}
x_{0,0} &amp; x_{0,1} &amp; x_{0,2} \\
x_{1,0} &amp; x_{1,1} &amp; x_{1,2} \\
x_{2,0} &amp; x_{2,1} &amp; x_{2,2} 
\end{bmatrix} \\
\begin{bmatrix}
x_{0,0} &amp; x_{0,1} &amp; x_{0,2} \\
x_{1,0} &amp; x_{1,1} &amp; x_{1,2} \\
x_{2,0} &amp; x_{2,1} &amp; x_{2,2} 
\end{bmatrix}
\end{bmatrix}
=
\begin{bmatrix}
\begin{bmatrix}
\vec{x}_0 \\
\vec{x}_1 \\
\vec{x}_2 
\end{bmatrix} \\
\begin{bmatrix}
\vec{x}_0 \\
\vec{x}_1 \\
\vec{x}_2 
\end{bmatrix} \\
\begin{bmatrix}
\vec{x}_0 \\
\vec{x}_1 \\
\vec{x}_2 
\end{bmatrix} \\
\end{bmatrix}
=
\begin{bmatrix}
X_0 \\
X_1 \\
X_2 
\end{bmatrix}\]

<h3 id="saving-processed-data">Saving Processed Data</h3>

<p>The final processed data was saved into <code class="language-plaintext highlighter-rouge">.npy</code> files:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">X_input</code>: Contained the input sequences.</li>
  <li><code class="language-plaintext highlighter-rouge">Y_input</code>: Contained the corresponding language labels.</li>
</ul>

<div class="quote-box">
  <p><b>Network Setup and Training</b></p>
</div>

<p>The core of this project is the LSTM network, designed to process sequences of text data. The network architecture consists of an embedding layer, followed by two LSTM layers, and dense layers for classification.</p>

<h3 id="network-structure">Network Structure</h3>

<ul>
  <li><strong>Embedding Layer:</strong> Converts input tokens into dense vectors of fixed size.</li>
  <li><strong>LSTM Layers:</strong> Two LSTM layers, the first with 128 cells and the second with 64 cells, process the embeddings and capture temporal dependencies in the text sequences.</li>
  <li><strong>Dense Layer:</strong> Outputs the probability distribution over the possible languages using a softmax activation function.</li>
</ul>

<h3 id="training-process">Training Process</h3>

<p>The network was trained using a categorical cross-entropy loss function and the Adam optimizer. The performance of the model was monitored on the validation set to prevent overfitting. Training and validation accuracy metrics were tracked to evaluate the model’s learning progress.</p>

<p>Here are the results after 14 epochs of training:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 14/15
782/782 <span class="o">[==============================]</span> - 91s 116ms/step - loss: 0.1281 - accuracy: 0.9647 - val_loss: 0.1356 - val_accuracy: 0.9590
</code></pre></div></div>
<p>That’s 96% accuracy.</p>

<div class="quote-box">
  <p><b>Example Interaction</b></p>
</div>

<p>After training, the tokenized map and the model were saved for easy import in future sessions. When an input sentence is provided, the program takes a sequence of 30 characters, tokenizes them according to the saved mapping, and uses the trained model to predict the language category.</p>

<p>Let’s see the model in action with an example:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> Enter your sentence:
<span class="o">&gt;&gt;&gt;</span> Successful coaches at powerhouses have traditionally stayed put, <span class="s2">"lording over fiefdoms until they lost their winning magic or were undone by age or scandal,"</span>
<span class="o">&gt;&gt;&gt;</span> It is English
</code></pre></div></div>

<div class="quote-box">
  <p><b>Conclusion</b></p>
</div>

<p>Key points include:</p>
<ul>
  <li><strong>Examining Original Data Format:</strong> Understanding the original format of the eBooks was crucial for identifying useful data sections, such as distinguishing headers and footers from the main content.</li>
  <li><strong>Manual Data Cleaning:</strong> Manually removing unlabeled data and non-standard book names ensured the integrity of the dataset, which is vital for training an accurate model.</li>
  <li><strong>Balancing the Dataset:</strong> Selecting the top six languages helped maintain a balanced dataset, which is essential for preventing bias and ensuring model generalizability.</li>
  <li><strong>Efficient Data Structuring:</strong> Tokenizing sequences into a 3D tensor using one-hot encoding was pivotal for preparing the data for the LSTM network.</li>
  <li><strong>Understanding LSTM Foundations:</strong> While LSTM is relatively old compared to current popular NLP models like Transformers, it forms the foundation of most modern NLP models. Understanding and implementing LSTM helps in grasping the internal mechanisms of how machines work with sequential data.</li>
  <li><strong>Code Repository:</strong> The complete code for this project is available on <a href="https://github.com/henryli121/language-identifier">GitHub</a>.</li>
</ul>]]></content><author><name></name></author><category term="research" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry xml:lang="en"><title type="html">Proofs for the Four Fundamental Equations of the Backpropagation and Algorithms in Feedforward Neural Networks</title><link href="http://localhost:4000/henry.github.io/paper/2024/06/18/01/" rel="alternate" type="text/html" title="Proofs for the Four Fundamental Equations of the Backpropagation and Algorithms in Feedforward Neural Networks" /><published>2024-06-18T22:40:00+08:00</published><updated>2024-06-18T22:40:00+08:00</updated><id>http://localhost:4000/henry.github.io/paper/2024/06/18/01</id><content type="html" xml:base="http://localhost:4000/henry.github.io/paper/2024/06/18/01/"><![CDATA[]]></content><author><name></name></author><category term="paper" /><summary type="html"><![CDATA[]]></summary></entry><entry xml:lang="zh"><title type="html">什么是音乐？</title><link href="http://localhost:4000/henry.github.io/blog/2024/06/07/01/" rel="alternate" type="text/html" title="什么是音乐？" /><published>2024-06-07T16:00:00+08:00</published><updated>2024-06-07T16:00:00+08:00</updated><id>http://localhost:4000/henry.github.io/blog/2024/06/07/01</id><content type="html" xml:base="http://localhost:4000/henry.github.io/blog/2024/06/07/01/"><![CDATA[<div class="final-quote">
音，声也。生于心，有节于外，谓之音。
<br />
<p style="font-size:13px;" align="right">  - 许慎《说文解字·音部》 </p> 
</div>
<p>“音是声音的一种。产生于内心，有节奏地表现出来后，叫做音乐。” 这是许慎在东汉时期就给出的定义。“生于心”，听着挺玄乎，但貌似有道理。其实，音乐的起源一直是一个谜，现在主流的说法主要有三种。</p>

<div class="quote-box">
  <p>音乐的起源</p>
</div>
<p><b>语言→音乐：</b>
<br />支持语言产生音乐的主要是考古学家。他们通过古代遗迹发现文字普遍出现的更早，因此，由于语言的表达方式不同，有的表达渐渐地演化成了我们所说的音乐。
<br />
<b>音乐→语言：</b>
<br />支持音乐产生语言的主要是人类学家。他们通过研究古老的部落发现，尽管一些部落还没有完整的语言系统，但是已经产生了通过音乐交流的途径。
<br />
最后一种说法就是：音乐和语言同时产生。</p>
<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0607_01.JPG" alt="Image 1" />
</div>

<div class="quote-box">
  <p>音乐和语言的差别</p>
</div>
<p>语言和音乐存在很大的不同，其中两个最大的差别就是<b>方向性</b>和<b>重复性</b>。</p>

<ul>
  <li>方向性：
语言有明显的方向性。一句完整的话有特定的主语和宾语，这明确表明了人物的指向。而且，语言要在两个人都会同一门语言的前提下，传递信息的功能才能实现。相反，音乐则没有明确的方向性。任何人都可以不同的形式和不同的理解接受一段旋律。正所谓：音乐无国界。<br /></li>
  <li>重复性:
语言和音乐都有重复，但是他们的功能完全不同。语言的重复是发生在当对方没有听懂和想强调自己的话时。但是音乐的重复目的是在<b>为了扩大情感的共鸣</b>。</li>
</ul>
<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0607_02.JPG" alt="Image 2" />
</div>

<div class="quote-box">
  <p>音乐和语言的功能</p>
</div>
<p>所以，简单的总结一下， 语言的功能是：<b>传递信息</b>；音乐的功能是：<b>传递感情</b>。<br />
那现在不妨让我们把思维再打开一些：<br />
什么是传递信息？无非就是一个物体产生信息，然后另一个物体接受信息。并且这只存在：0（未接受）和1（接受）这两种可能性。比如：当我们使用电脑，这就是一种信息的传递，而我们使用的就是计算机语言：0和1。因此，基于程序的人工智能在过去的几年便轻松攻破了人类的语言体系。<br />
什么是传递感情？这个问题显然宽泛。但是，如果我们想象信息是一种0和1的状态话，不难想象，情感就是0和1的<b>纠缠态</b>。比如，我们不会说我完全接受/未接受这个感情。但是，我们会说我又爱你，又恨你。这就是感情的不确定性。因此，即便如今的人工智能可以根据乐理来创作音乐，但是在人类看来那始终是缺乏情感的。</p>
<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0607_03.JPG" alt="Image 3" />
</div>

<div class="quote-box">
  <p>音乐是什么</p>
</div>
<p>最后，我不妨回答我们文章最开始的那个问题：音乐是什么？<br />
如果说音乐的功能是为了传递纠缠态的感情，那么音乐一定是更高纬度的产物。就像我们现在还无法理解量子纠缠一样。而我们所说的歌曲，旋律，仅仅是音乐在我们三维世界上的投影。所以，<b>现在人类还无法完全的理解音乐</b>。<br />
但是，我们人类有个有意思的特点，那就是当我们无法解释一个现象时，把它推给上帝，一切就解释得通了。在大多数宗教里都有天使这个概念：天使本意为上天的使者。上帝的使者，他也是离上帝最近的人。而天使的作用就是将神的讯息传递给人类，并且记录下人类的活动然后汇报给上帝。但是，为什么上帝不自己给人类说呢，一定要让天使说呢？因为人类根本无法理解上帝的语言，上帝生活在更高的纬度。因此，我们需要天使这个翻译，来翻译上帝的语言，而这个无法理解的高纬度语言就是音乐。<br /></p>
<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0607_04.JPG" alt="Image 4" />
</div>
<p>最后再来解读柏拉图这句对音乐看法：“<b>音乐是道德的法则。它给予宇宙灵魂，思想翅膀，想象飞翔，赋予生活欢乐和一切。</b>”也许，这就是上帝的意义吧。</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[音，声也。生于心，有节于外，谓之音。 - 许慎《说文解字·音部》 “音是声音的一种。产生于内心，有节奏地表现出来后，叫做音乐。” 这是许慎在东汉时期就给出的定义。“生于心”，听着挺玄乎，但貌似有道理。其实，音乐的起源一直是一个谜，现在主流的说法主要有三种。]]></summary></entry><entry xml:lang="zh"><title type="html">从零开始创建个人网站的旅程</title><link href="http://localhost:4000/henry.github.io/create/2024/05/30/website-creation/" rel="alternate" type="text/html" title="从零开始创建个人网站的旅程" /><published>2024-05-30T17:00:00+08:00</published><updated>2024-05-30T17:00:00+08:00</updated><id>http://localhost:4000/henry.github.io/create/2024/05/30/website-creation</id><content type="html" xml:base="http://localhost:4000/henry.github.io/create/2024/05/30/website-creation/"><![CDATA[<h3 id="引言">引言</h3>

<p>欢迎来到我的创作页面！在这篇博文中，我将与大家分享我是如何从零开始创建这个网站的全过程。希望这段经历能对那些也在尝试自己动手搭建网站的朋友们有所帮助。</p>

<div class="quote-box">
  <p>Github Pages 网站设置</p>
</div>

<p><strong>选择平台与框架</strong></p>
<ul>
  <li>经过一番研究，我决定使用 GitHub Pages 和 Jekyll 来搭建我的个人网站。Jekyll 是一个简单、博客友好的静态网站生成器，非常适合像我这样的初学者。</li>
</ul>

<p><strong>初始设置</strong></p>
<ul>
  <li>首先，我在 GitHub 上创建了一个新的仓库，并克隆到本地。使用以下命令完成克隆：
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/henryli121/henry.github.io.git
</code></pre></div>    </div>
  </li>
  <li>接着，我安装了 Jekyll 和 Bundler，通过命令行工具初始化了 Jekyll 项目：
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem <span class="nb">install </span>jekyll bundler
jekyll new my-awesome-site
<span class="nb">cd </span>my-awesome-site
bundle <span class="nb">exec </span>jekyll serve
</code></pre></div>    </div>
  </li>
  <li>在初始化过程中，我遇到了 gem 安装失败的问题，通过更新 RubyGems 和使用 sudo 命令解决了这个问题：
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem update <span class="nt">--system</span>
<span class="nb">sudo </span>gem <span class="nb">install </span>jekyll bundler
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>初次构建</strong></p>
<ul>
  <li>使用 Jekyll 生成的默认模板，我成功地构建了第一个版本的网站，并通过 GitHub Pages 发布到网上：
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div>    </div>
  </li>
  <li>在构建过程中，我遇到了 Jekyll 版本不兼容的问题，通过指定特定版本的 Jekyll 解决了这个问题：
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem <span class="s1">'jekyll'</span>, <span class="s1">'~&gt; 4.2.0'</span>
bundle <span class="nb">install</span>
</code></pre></div>    </div>
  </li>
</ul>

<div class="quote-box">
  <p>博客创建部分</p>
</div>

<p><strong>配置 _config.yml</strong></p>
<ul>
  <li>网站初次构建成功后，我开始修改 <code class="language-plaintext highlighter-rouge">_config.yml</code> 文件，以自定义网站的基本信息，包括站点标题、描述、导航链接等。</li>
</ul>

<p><strong>目录结构调整</strong></p>
<ul>
  <li>按照 Jekyll 的规范，我调整了项目的目录结构，确保所有文件和资源都在正确的位置上。以下是我的当前目录结构：</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.
├── _config.yml
├── _site
├── _includes
│   ├── head.html
│   └── footer.html
├── _layouts
│   ├── base.html
│   ├── home.html
│   ├── page.html
│   └── post.html
├── _posts
│   └── 2024-05-26-test.md
├── assets
│   ├── blog_images
│   ├── css
│   │   ├── gallery.css
│   │   ├── lightbox.css
│   │   ├── main.css
│   │   ├── main.css.map
│   ├── images
│   └── js
├── pic.md
├── about.md
├── blog.md
├── creation.md
└── other files
</code></pre></div></div>

<p><strong>版本控制</strong></p>
<ul>
  <li>每次做出修改后，我都会使用 Git 进行版本控制，并将更新推送到 GitHub 上。这样可以方便地追踪历史变更，并在必要时恢复到之前的版本：
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add <span class="nb">.</span>
git commit <span class="nt">-m</span> <span class="s2">"Initial commit"</span>
git push origin main
</code></pre></div>    </div>
  </li>
</ul>

<div class="quote-box">
  <p>使用 Lightbox2 实现图片展示</p>
</div>

<p><strong>引入 Lightbox2</strong></p>
<ul>
  <li>为了更好地展示图片，我决定使用 Lightbox2 插件。这个插件可以让用户点击图片时显示大图，并添加图片标题。</li>
</ul>

<p><strong>安装与配置</strong></p>
<ul>
  <li>我按照 Lightbox2 的文档，将相关的 CSS 和 JS 文件引入到项目中。然而，在实现过程中遇到了标题无法正常显示的问题。经过一番排查，发现是由于 Lightbox2 的某些配置与我的项目结构不兼容。最终，通过添加 jQuery 解决了这个问题：
    <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://code.jquery.com/jquery-3.6.0.min.js"</span><span class="nt">&gt;&lt;/script&gt;</span>
</code></pre></div>    </div>
  </li>
</ul>

<div class="quote-box">
  <p>调试与优化</p>
</div>

<p><strong>调试工具的使用</strong></p>
<ul>
  <li>在开发过程中，我频繁使用浏览器的开发者工具（如 Chrome DevTools）来调试和检查页面元素、样式以及脚本错误。这对定位问题和优化性能非常有帮助。</li>
</ul>

<p><strong>常见错误与解决方案</strong></p>

<p><strong>目录结构问题</strong></p>
<ul>
  <li>
    <p>项目目录结构不清晰可能导致资源文件无法正确加载。确保所有文件和资源都在正确的位置上，如 <code class="language-plaintext highlighter-rouge">_includes</code> 文件夹中的 <code class="language-plaintext highlighter-rouge">head.html</code> 和 <code class="language-plaintext highlighter-rouge">footer.html</code>，以及 <code class="language-plaintext highlighter-rouge">_layouts</code> 文件夹中的其他 HTML 文件。</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">_site</code> 文件夹用于存储 Jekyll 构建后的静态网站文件，每次构建时都会被重新生成。因此，任何修改都应该在源文件中完成，而不是直接修改 <code class="language-plaintext highlighter-rouge">_site</code> 中的文件。</p>
  </li>
</ul>

<p><strong>资源文件路径</strong></p>
<ul>
  <li>在使用资源文件（如图片、CSS、JS）时，确保使用相对路径，这样在本地和 GitHub Pages 上都能正常工作。例如：
    <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;link</span> <span class="na">rel=</span><span class="s">"stylesheet"</span> <span class="na">href=</span><span class="s">"/henry.github.io/assets/css/main.css"</span><span class="nt">&gt;</span>
<span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"/henry.github.io/assets/js/lightbox.js"</span><span class="nt">&gt;&lt;/script&gt;</span>
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>其他问题</strong></p>
<ul>
  <li>文件路径错误：确保所有资源文件都在正确的路径下，避免使用绝对路径。</li>
  <li>缓存问题：每次更新后清除浏览器缓存，确保加载到的是最新版本的文件：
    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll clean
bundle <span class="nb">exec </span>jekyll serve
</code></pre></div>    </div>
  </li>
  <li>兼容性问题：检查不同浏览器下的显示效果，确保网站在主流浏览器中都能正常工作。</li>
</ul>

<p><strong>遇到的其他问题</strong></p>
<ul>
  <li>在实现一些特定功能时，比如创建博客页面和添加社交媒体按钮，遇到了样式问题和 JavaScript 错误。通过不断调试和查阅文档，最终解决了这些问题。</li>
</ul>

<h3 id="复盘">复盘</h3>
<p>其实，在使用 Jekyll 搭建我的个人网站之前，我也尝试过市面上其他的平台，比如 WordPress 和 Wix，但它们都有进阶内容付费、自主性低的问题。所以我决定从源代码开始写网站。通过这次从零开始的搭建经历，我最感到吃惊的是现在编程学习或者网络技术学习的成本之低。</p>

<p>在开始这个项目之前，我从未接触过 HTML 和 CSS。于是抱着尝试的态度，我开始了这个项目。当然，敢于继续下去的另一个原因是 <b>ChatGPT-4o</b>。毫不夸张地说，这个网页80%的代码都是 ChatGPT 帮我写的，它就像一个得力的助手，虽然称不上老师，但已足够优秀。当然，它不可能一口气全部帮你写出来，更多的是你和它的合作。在这个项目中，我关注的是整体的逻辑步骤，比如：如何搭建一个最基本的框架，然后逐步加入细节的调整，再进行优化和美化。我需要把这些步骤拆分成一个个小任务，再让 ChatGPT 来帮我写。一个个小任务的完成，最终搭建出了整个网站。</p>

<p>相对于几十年前动辄几个月的编程课程，现在在 AI 技术的加持下，编程学习或者网络技术学习的成本已经非常低。到网站目前这个状态，一共也就花费了我两天的时间。甚至这篇博文的大部分内容也是 ChatGPT 帮我总结的，因为它能够记住从项目开始以来我们之间的对话。因此，我只需要告诉它：“帮我把这个项目的过程写成一篇博客，名字叫‘从零开始创建个人网站的旅程’。”于是，15秒的时间这篇文章就写好了。至于哪些部分是我写的，就留给你们去猜测吧。</p>

<h3 id="结语">结语</h3>

<p>创建这个网站的过程充满了挑战与乐趣。从最初的框架选择，到中途的各种问题排查，再到最后的调试优化，每一步都是一次学习和成长的机会。希望通过这篇博文，能为同样在搭建网站道路上的你提供一些启发和帮助。</p>

<p>如果你在网站创建过程中遇到了类似的问题，欢迎留言，我们可以一起讨论和解决。谢谢阅读！</p>]]></content><author><name></name></author><category term="create" /><summary type="html"><![CDATA[引言]]></summary></entry><entry xml:lang="zh"><title type="html">Més que un Club 不仅仅是一家俱乐部：巴塞罗那的善行与价值观</title><link href="http://localhost:4000/henry.github.io/blog/2024/05/30/02/" rel="alternate" type="text/html" title="Més que un Club 不仅仅是一家俱乐部：巴塞罗那的善行与价值观" /><published>2024-05-30T10:00:00+08:00</published><updated>2024-05-30T10:00:00+08:00</updated><id>http://localhost:4000/henry.github.io/blog/2024/05/30/02</id><content type="html" xml:base="http://localhost:4000/henry.github.io/blog/2024/05/30/02/"><![CDATA[<p>2022.06.30 — 巴塞罗那俱乐部财政赤字：<b>-6.08</b>亿欧元
<br />
<br />
2022.09.03 — 巴塞罗那连续第14个赛季向联合国儿童基金会捐赠<b>150万</b>欧元
<br />
<br />
2023.06.23 — 巴塞罗那俱乐部依旧财政赤字：<b>-13.5亿</b>欧元
<br />
<br />
2023.09.11 — 巴塞罗那俱乐部通过基金会向摩洛哥地震施予援助。</p>

<div class="quote-box">
  <h2>Més que un Club</h2>
  <p>不仅仅是一家俱乐部</p>
</div>
<p>在伊比利亚的心灵深处，巴塞罗那这座古老的城市，如诗如画地融合了历史与文化。而那同名的足球俱乐部，不仅仅是竞技场上的辉煌，它是一首赞歌，是一种信仰。虽有人仅见其为足球的舞台，但真正的球迷知晓，我们所守护的，是一种超越比赛的精神，是一种生活的哲学。
<br />
<br />
<b>不仅仅是足球</b>（282座冠军）：
我们是一个多项运动俱乐部，在足球（79座冠军）、篮球（47座冠军）、手球（64座冠军）、轮滑曲棍球（78座冠军）和五人制足球（14座冠军）中都在最高水平上竞争。
<br />
<br />
<b>不仅仅是男足</b>：
我们坚定地致力于女子运动。我们拥有一个专业的女子足球队（36座冠军），过去4年内3次站在欧洲之巅。我们的五个训练队，以及总共663名女孩和女性，在俱乐部的9个业余运动中穿着蓝红色球衣。
<br />
<br />
<b>我们不仅仅竞技</b>：
我们是一个通过体育教育人们的“生活学校”。我们不仅想吸引最优秀的球员，还希望利用拉玛西亚培养出在场上和场下都被认可和有贡献的优秀人才。
<br />
<br />
<b>我们不仅仅来自加泰</b>：
我们一直对外界保持开放，并已成为不同人群、文化和国家的交汇之地。我们更是一个俱乐部，在这里，各种文化得到了真正的交融和尊重，我们坚信每一种文化都有其独特之处，都值得被尊重和欣赏。</p>
<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0530_01.JPG" alt="Image 1" />
  <br />
  <div class="caption">诺坎普球场</div>
</div>

<div class="quote-box">
  <p>内在的价值</p>
</div>
<p>比起那些背的烂熟于心的成就，这十几年来，巴塞罗那带给我的更多的是如何保持<b>尊重</b>、<b>努力</b>、<b>雄心</b>、<b>团队合作</b>和<b>谦逊</b>这五大核心价值观。
<br />
<br />
2006年 — 巴塞罗那放弃<b>1.5亿</b>欧元胸前广告赞助，将联合国儿童基金会标志印在胸前，并每个赛季提供<b>150万</b>欧元的捐款。
<br />
<br />
2009年 — 巴塞罗那俱乐部所有职业运动员和教练自愿将收入的<b>0.5%</b>捐赠给基金会。
<br />
<br />
2017年 — 沙佩科恩空难，巴塞罗那全欧洲唯一一家俱乐部与其进行慈善赛，并捐款<b>25万</b>欧元。
<br />
<br />
2020年 — 新冠爆发，巴萨陷入财政赤字，但依旧捐款<b>50万</b>欧元来帮助研究对抗新冠病毒。</p>
<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0530_02.JPG" alt="Image 2" />
  <br />
  <div class="caption">巴萨基金会</div>
</div>

<div class="quote-box">
  <p>我爱的不仅仅是冠军</p>
</div>
<p>或许，在尘世的奖杯数上，我们与白色的俱乐部相比，似乎不那么“伟大”。但这要看你如何定义伟大。一座冠军奖杯，自然是荣耀。然而，在这起伏的世俗间，那份无畏苦难，持续行善的本心，已是无上的光辉。巴塞罗那始终守护着那份内心的善心与智慧，正如它对传控足球的坚定信仰。<b>因为我所珍视的，不仅仅是那刹那的荣光，更是它带给我无愧于心的内在价值。</b></p>

<div class="final-quote">
  “行善者，不畏困苦，不计前功，不望后福，但求无愧于心。” 
</div>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[2022.06.30 — 巴塞罗那俱乐部财政赤字：-6.08亿欧元 2022.09.03 — 巴塞罗那连续第14个赛季向联合国儿童基金会捐赠150万欧元 2023.06.23 — 巴塞罗那俱乐部依旧财政赤字：-13.5亿欧元 2023.09.11 — 巴塞罗那俱乐部通过基金会向摩洛哥地震施予援助。]]></summary></entry><entry xml:lang="zh"><title type="html">拍摄星空 是件浪漫又孤独的事</title><link href="http://localhost:4000/henry.github.io/blog/2024/05/29/01/" rel="alternate" type="text/html" title="拍摄星空 是件浪漫又孤独的事" /><published>2024-05-29T20:00:00+08:00</published><updated>2024-05-29T20:00:00+08:00</updated><id>http://localhost:4000/henry.github.io/blog/2024/05/29/01</id><content type="html" xml:base="http://localhost:4000/henry.github.io/blog/2024/05/29/01/"><![CDATA[<p>本科的时候，我上了一节 “观测天文学” 的物理课。那节课重新激发了我对宇宙的兴趣。小时候的梦想一直的梦想就是做一个天文学家，但是随着年龄的长大，我发现，宇宙实在是太远太大了，总感觉这个梦想不切实际。</p>
<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0529_01.JPG" alt="Image 1" />
  <br />
  <div class="caption"> 图中的蓝点是旅行者1号在离开太阳系时拍摄的地球</div>
</div>

<p>但是心里那种对宇宙的好奇和对未知的探索，却一直推动者我。抬起头，去仰望这个一望无际的宇宙；去拍摄，这星光璀璨的星空。</p>
<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0529_02.JPG" alt="Image 2" />
</div>
<div class="image-container">
    <img src="/henry.github.io/assets/blog_images/0529_04.JPG" alt="Image 3" />
    <br />
    <div class="caption">60倍镜头变焦下的月亮</div>
</div>

<p>月球是我拍摄过最多的天体。也许是因为她最近，也许是因为她亮，但是更吸引我的是<b>她那永远看不到的背面</b>。</p>

<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0529_05.JPG" alt="Image 4" />
  <br />
  <div class="caption">天文课上用专业的望远镜拍摄的NGC 4485星系，距地球2500万光年</div>
</div>

<p>这是我拍摄过最远的星体。虽然在一瞬间就看了它，但是这个星体发出的光其实是2500万年前的。那时候地球还在中新世，冰河时代即将开始，哺乳动物也才刚刚出现。
<br />
<br />
有人说当你抬头仰望星空时，你就能看见整个宇宙的历史。的确，因为光速是有限的，你看的月亮其实是1.3秒前的月亮，你看到的太阳其实是8分钟前的太阳。星体越远，你看到是它们越遥远的历史。</p>

<div class="image-container">
  <img src="/henry.github.io/assets/blog_images/0529_06.JPG" alt="Image 5" />
  <br />
  <div class="caption">昨晚我头顶的星空</div>
</div>

<p>我望着这星河璀璨的夜空，突然感到有一阵无助的孤独。<b>我</b> 只是这个地球上的1/70亿，地球只是银河系1/1000亿，而银河系只是这个宇宙的1/10000亿。
<br />
<br />
我 小到忽略不计，于是我问自己，“<b>我存在的意义又是什么呢？</b>”
<br />
<br />
又突然想到，当1000年前的苏轼写下 “<b>寄蜉蝣于天地，渺沧海之一粟</b>” 时，他是不是跟我仰望的同一片星空呢？</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[本科的时候，我上了一节 “观测天文学” 的物理课。那节课重新激发了我对宇宙的兴趣。小时候的梦想一直的梦想就是做一个天文学家，但是随着年龄的长大，我发现，宇宙实在是太远太大了，总感觉这个梦想不切实际。 图中的蓝点是旅行者1号在离开太阳系时拍摄的地球]]></summary></entry></feed>