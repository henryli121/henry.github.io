---
layout: post-en
title: "Training a Machine to Learn Different Languages"
date: 2024-06-19 20:00:00 +0800
categories: research
lang: "en"
---

<div class="quote-box">
  <p><b>Introduction</b></p>
</div>

Understanding and identifying languages from text inputs is a fundamental task in natural language processing, with applications in automatic translation and multilingual sentiment analysis. This project aims to develop a machine learning model to accurately identify the language of a given text snippet. We leverage Long Short-Term Memory (LSTM) networks, a type of recurrent neural network known for capturing long-term dependencies in sequential data. LSTMs maintain context over extended text sequences, making them ideal for understanding and processing human languages. This post details the process from data preparation and cleaning to training an LSTM-based model.

<div class="quote-box">
  <p><b>Model Architecture: Long Short-Term Memory (LSTM)</b></p>
</div>

Long Short-Term Memory (LSTM) networks are designed to address the vanishing gradient problem inherent in traditional RNNs, making them effective for handling long-term dependencies in sequential data.

The architecture of an LSTM cell involves a gating mechanism that includes a forget gate, an input gate, and an output gate. These gates regulate information flow, enabling the cell to maintain and update information over long sequences.

The internal operations of an LSTM cell can be described by the following equations:

1. **Forget Gate:** Determines which information from the previous cell state to discard:
   $$
   f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
   $$
   
2. **Input Gate:** Decides which new information to add to the cell state:
   $$
   i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
   $$
   $$
   \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
   $$
   
3. **Cell State Update:** Combines the old cell state, modified by the forget gate, with the new candidate values, scaled by the input gate:
   $$
   C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
   $$
   
4. **Output Gate:** Decides the next hidden state based on the updated cell state:
   $$
   o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
   $$
   $$
   h_t = o_t \cdot \tanh(C_t)
   $$

In these equations:
- $$ \sigma $$ represents the sigmoid activation function.
- $$ \tanh $$ denotes the hyperbolic tangent function.
- $$ W $$ and $$ b $$ are the weight matrices and biases.
- $$ h_t $$ and $$ C_t $$ represent the hidden state and cell state at time step $$ t $$.

<div class="image-container">
  <img src="{{ '/assets/blog_images/0620_1.JPG' | relative_url }}" alt="Image 1">
   <br>
   <div class="caption">Figure 1: Diagram illustration of LSTM</div>
</div>

LSTMs excel in language tasks due to their ability to retain context over extended sequences, making them a preferred choice for NLP applications such as language modeling, translation, and sentiment analysis.

<div class="quote-box">
  <p><b>Data Collection and Cleaning</b></p>
</div>

The initial step involved preparing a dataset comprising text samples from various languages. Ensuring the data was clean and well-structured was crucial for the model's performance.

The data cleaning process involved identifying and removing unlabeled data, handling books with unusual names, stripping prefaces and postfaces, and filtering outliers. After cleaning, the dataset was balanced by selecting the top six languages and structured into a 3D tensor. The final processed data was saved in `.npy` files for efficient training.

<div class="image-container">
  <img src="{{ '/assets/blog_images/0620_2.JPG' | relative_url }}" alt="Image 2">
   <br>
   <div class="caption">Figure 2: The format of an ebook</div>
</div>

### Data Cleaning Process

Several critical steps were involved to ensure the dataset's integrity and usability:

1. **Identification and Removal of Unlabeled Data:** Manually identifying and removing books without clear language tags.
2. **Handling Unusual Names:** Manually reviewing books with non-standard naming formats to maintain consistency.
3. **Stripping Prefaces and Postfaces:** Removing prefaces and postfaces typically written in English to prevent language contamination.
4. **Filtering Outliers:** Removing books that did not align with the typical format.

After these steps, the dataset was reduced to 36,082 books. To ensure balanced training, the top six languages with the highest number of samples were selected.

### Data Distribution and Structure

Post-cleaning, the dataset was structured into a 3D tensor. For each language and each book, random sequences of 30 consecutive characters were extracted and tokenized using one-hot encoding.

The 3D tensor can be represented as follows:

$$
\mathcal{X} = 
\begin{bmatrix}
\begin{bmatrix}
x_{0,0} & x_{0,1} & x_{0,2} \\
x_{1,0} & x_{1,1} & x_{1,2} \\
x_{2,0} & x_{2,1} & x_{2,2} 
\end{bmatrix} \\
\begin{bmatrix}
x_{0,0} & x_{0,1} & x_{0,2} \\
x_{1,0} & x_{1,1} & x_{1,2} \\
x_{2,0} & x_{2,1} & x_{2,2} 
\end{bmatrix} \\
\begin{bmatrix}
x_{0,0} & x_{0,1} & x_{0,2} \\
x_{1,0} & x_{1,1} & x_{1,2} \\
x_{2,0} & x_{2,1} & x_{2,2} 
\end{bmatrix}
\end{bmatrix}
=
\begin{bmatrix}
\begin{bmatrix}
\vec{x}_0 \\
\vec{x}_1 \\
\vec{x}_2 
\end{bmatrix} \\
\begin{bmatrix}
\vec{x}_0 \\
\vec{x}_1 \\
\vec{x}_2 
\end{bmatrix} \\
\begin{bmatrix}
\vec{x}_0 \\
\vec{x}_1 \\
\vec{x}_2 
\end{bmatrix} \\
\end{bmatrix}
=
\begin{bmatrix}
X_0 \\
X_1 \\
X_2 
\end{bmatrix}
$$

### Saving Processed Data

The final processed data was saved into `.npy` files:
- `X_input`: Contained the input sequences.
- `Y_input`: Contained the corresponding language labels.

<div class="quote-box">
  <p><b>Network Setup and Training</b></p>
</div>

The core of this project is the LSTM network, designed to process sequences of text data. The network architecture consists of an embedding layer, followed by two LSTM layers, and dense layers for classification.

### Network Structure

- **Embedding Layer:** Converts input tokens into dense vectors of fixed size.
- **LSTM Layers:** Two LSTM layers, the first with 128 cells and the second with 64 cells, process the embeddings and capture temporal dependencies in the text sequences.
- **Dense Layer:** Outputs the probability distribution over the possible languages using a softmax activation function.

### Training Process

The network was trained using a categorical cross-entropy loss function and the Adam optimizer. The performance of the model was monitored on the validation set to prevent overfitting. Training and validation accuracy metrics were tracked to evaluate the model's learning progress.

Here are the results after 14 epochs of training:
```sh
Epoch 14/15
782/782 [==============================] - 91s 116ms/step - loss: 0.1281 - accuracy: 0.9647 - val_loss: 0.1356 - val_accuracy: 0.9590
```
That’s 96% accuracy.

<div class="quote-box">
  <p><b>Example Interaction</b></p>
</div>

After training, the tokenized map and the model were saved for easy import in future sessions. When an input sentence is provided, the program takes a sequence of 30 characters, tokenizes them according to the saved mapping, and uses the trained model to predict the language category.

Let’s see the model in action with an example:
```sh
>>> Enter your sentence:
>>> Successful coaches at powerhouses have traditionally stayed put, "lording over fiefdoms until they lost their winning magic or were undone by age or scandal,"
>>> It is English
```

<div class="quote-box">
  <p><b>Conclusion</b></p>
</div> 

Key points include:
- **Examining Original Data Format:** Understanding the original format of the eBooks was crucial for identifying useful data sections, such as distinguishing headers and footers from the main content.
- **Manual Data Cleaning:** Manually removing unlabeled data and non-standard book names ensured the integrity of the dataset, which is vital for training an accurate model.
- **Balancing the Dataset:** Selecting the top six languages helped maintain a balanced dataset, which is essential for preventing bias and ensuring model generalizability.
- **Efficient Data Structuring:** Tokenizing sequences into a 3D tensor using one-hot encoding was pivotal for preparing the data for the LSTM network.
- **Understanding LSTM Foundations:** While LSTM is relatively old compared to current popular NLP models like Transformers, it forms the foundation of most modern NLP models. Understanding and implementing LSTM helps in grasping the internal mechanisms of how machines work with sequential data.
- **Code Repository:** The complete code for this project is available on [GitHub](https://github.com/henryli121/language-identifier).